F:\Anaconda\envs\pytorch\python.exe F:/NLP大作业/SimCSE-Pytorch-master/ESimCSE/tongyi_distill_train.py
2024-12-23 16:38:46.044 | INFO     | __main__:<module>:499 - Starting training process with knowledge distillation from Tongyi embeddings.
2024-12-23 16:38:46.044 | INFO     | __main__:<module>:500 - Namespace(batch_size=16, data_path='../data/STS-B/', device='cuda:0', dropout=0.15, dup_rate=0.15, lr=3e-05, max_length=50, pooler='first-last-avg', pretrain_model_path='F:\\models\\bert-base-chinese', q_size=64, save_path='./model_save', teacher_save_path='./cnsd_sts_train_unsup_embeddings_768.json')
Using cuda:0 device.

2024-12-23 16:38:46.333 | INFO     | __main__:<module>:505 - Test Embeddings长度: 1024
2024-12-23 16:38:46.359 | INFO     | __main__:main:424 - Generating/updating embeddings...
2024-12-23 16:38:48.554 | INFO     | __main__:generate_teacher_embeddings:166 - Total sentences: 10462, Remaining to embed: 0
Generating embeddings: 0it [00:00, ?it/s]
2024-12-23 16:38:48.556 | INFO     | __main__:generate_teacher_embeddings:193 - Embedding generation completed. Saved to ./cnsd_sts_train_unsup_embeddings_768.json
original_dim: 768
PCA: n_components: 768
2024-12-23 16:38:51.462 | INFO     | __main__:train_with_distillation:322 - Applying PCA to teacher embeddings...
2024-12-23 16:38:51.462 | INFO     | __main__:apply_pca:43 - PCA skipped because n_components (768) matches the input dimension (768).
Training:   0%|          | 0/654 [00:00<?, ?it/s]F:\Anaconda\envs\pytorch\lib\site-packages\transformers\models\bert\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
Training:   0%|          | 3/654 [00:45<2:08:46, 11.87s/it]2024-12-23 16:39:37.338 | INFO     | __main__:train_with_distillation:358 - Batch 5/654 - loss: 0.0528
Training:   0%|          | 3/654 [00:57<2:08:46, 11.87s/it]2024-12-23 16:40:24.088 | INFO     | __main__:train_with_distillation:364 - New best corrcoef: 0.5731 at batch 5, model saved.
Training:   1%|▏         | 9/654 [01:32<1:09:19,  6.45s/it]2024-12-23 16:40:24.455 | INFO     | __main__:train_with_distillation:358 - Batch 10/654 - loss: 0.0376
Training:   2%|▏         | 14/654 [02:19<1:04:43,  6.07s/it]2024-12-23 16:41:10.993 | INFO     | __main__:train_with_distillation:358 - Batch 15/654 - loss: 0.0276
Training:   2%|▏         | 14/654 [02:37<1:04:43,  6.07s/it]2024-12-23 16:41:58.086 | INFO     | __main__:train_with_distillation:364 - New best corrcoef: 0.5810 at batch 15, model saved.
Training:   3%|▎         | 19/654 [03:06<1:03:44,  6.02s/it]2024-12-23 16:41:58.457 | INFO     | __main__:train_with_distillation:358 - Batch 20/654 - loss: 0.0333
Training:   3%|▎         | 19/654 [03:17<1:03:44,  6.02s/it]2024-12-23 16:42:45.402 | INFO     | __main__:train_with_distillation:364 - New best corrcoef: 0.6004 at batch 20, model saved.
Training:   4%|▎         | 24/654 [03:54<1:02:59,  6.00s/it]2024-12-23 16:42:45.784 | INFO     | __main__:train_with_distillation:358 - Batch 25/654 - loss: 0.0242
Training:   4%|▎         | 24/654 [04:07<1:02:59,  6.00s/it]2024-12-23 16:43:33.340 | INFO     | __main__:train_with_distillation:364 - New best corrcoef: 0.6177 at batch 25, model saved.
Training:   4%|▍         | 29/654 [04:42<1:02:56,  6.04s/it]2024-12-23 16:43:33.708 | INFO     | __main__:train_with_distillation:358 - Batch 30/654 - loss: 0.0693
Training:   4%|▍         | 29/654 [04:57<1:02:56,  6.04s/it]2024-12-23 16:44:21.153 | INFO     | __main__:train_with_distillation:364 - New best corrcoef: 0.6214 at batch 30, model saved.
Training:   5%|▌         | 34/654 [05:29<1:02:28,  6.05s/it]2024-12-23 16:44:21.523 | INFO     | __main__:train_with_distillation:358 - Batch 35/654 - loss: 0.0212
Training:   5%|▌         | 34/654 [05:47<1:02:28,  6.05s/it]2024-12-23 16:45:09.767 | INFO     | __main__:train_with_distillation:364 - New best corrcoef: 0.6352 at batch 35, model saved.
Training:   6%|▌         | 39/654 [06:18<1:02:40,  6.11s/it]2024-12-23 16:45:10.160 | INFO     | __main__:train_with_distillation:358 - Batch 40/654 - loss: 0.0198
Training:   6%|▌         | 39/654 [06:37<1:02:40,  6.11s/it]2024-12-23 16:45:58.254 | INFO     | __main__:train_with_distillation:364 - New best corrcoef: 0.6394 at batch 40, model saved.
Training:   7%|▋         | 44/654 [07:07<1:02:17,  6.13s/it]2024-12-23 16:45:58.619 | INFO     | __main__:train_with_distillation:358 - Batch 45/654 - loss: 0.0183
Training:   7%|▋         | 44/654 [07:17<1:02:17,  6.13s/it]2024-12-23 16:46:46.914 | INFO     | __main__:train_with_distillation:364 - New best corrcoef: 0.6408 at batch 45, model saved.
Training:   7%|▋         | 49/654 [07:55<1:01:57,  6.15s/it]2024-12-23 16:46:47.283 | INFO     | __main__:train_with_distillation:358 - Batch 50/654 - loss: 0.0164
Training:   8%|▊         | 54/654 [08:44<1:01:13,  6.12s/it]2024-12-23 16:47:35.585 | INFO     | __main__:train_with_distillation:358 - Batch 55/654 - loss: 0.0151
Training:   9%|▉         | 59/654 [09:31<1:00:14,  6.08s/it]2024-12-23 16:48:23.413 | INFO     | __main__:train_with_distillation:358 - Batch 60/654 - loss: 0.0148
Training:  10%|▉         | 64/654 [10:19<59:49,  6.08s/it]  2024-12-23 16:49:11.532 | INFO     | __main__:train_with_distillation:358 - Batch 65/654 - loss: 0.0149
Training:  11%|█         | 69/654 [11:07<58:33,  6.01s/it]  2024-12-23 16:49:58.655 | INFO     | __main__:train_with_distillation:358 - Batch 70/654 - loss: 0.0129
Training:  11%|█▏        | 74/654 [11:55<58:36,  6.06s/it]  2024-12-23 16:50:46.817 | INFO     | __main__:train_with_distillation:358 - Batch 75/654 - loss: 0.0113
Training:  12%|█▏        | 79/654 [12:42<57:39,  6.02s/it]  2024-12-23 16:51:34.185 | INFO     | __main__:train_with_distillation:358 - Batch 80/654 - loss: 0.0114
Training:  13%|█▎        | 84/654 [13:30<57:29,  6.05s/it]  2024-12-23 16:52:22.177 | INFO     | __main__:train_with_distillation:358 - Batch 85/654 - loss: 0.0192
Training:  14%|█▎        | 89/654 [14:17<56:09,  5.96s/it]  2024-12-23 16:53:08.956 | INFO     | __main__:train_with_distillation:358 - Batch 90/654 - loss: 0.0093
Training:  14%|█▍        | 94/654 [15:04<55:23,  5.93s/it]  2024-12-23 16:53:55.742 | INFO     | __main__:train_with_distillation:358 - Batch 95/654 - loss: 0.0094
Training:  15%|█▌        | 99/654 [15:51<55:08,  5.96s/it]  2024-12-23 16:54:42.965 | INFO     | __main__:train_with_distillation:358 - Batch 100/654 - loss: 0.0079
Training:  16%|█▌        | 104/654 [16:39<55:36,  6.07s/it]  2024-12-23 16:55:31.356 | INFO     | __main__:train_with_distillation:358 - Batch 105/654 - loss: 0.0074
Training:  17%|█▋        | 109/654 [17:27<54:54,  6.04s/it]  2024-12-23 16:56:19.042 | INFO     | __main__:train_with_distillation:358 - Batch 110/654 - loss: 0.0069
Training:  17%|█▋        | 114/654 [18:14<54:09,  6.02s/it]  2024-12-23 16:57:06.508 | INFO     | __main__:train_with_distillation:358 - Batch 115/654 - loss: 0.0060
Training:  18%|█▊        | 119/654 [19:02<53:32,  6.00s/it]  2024-12-23 16:57:53.906 | INFO     | __main__:train_with_distillation:358 - Batch 120/654 - loss: 0.0057
Training:  19%|█▉        | 124/654 [19:49<52:59,  6.00s/it]  2024-12-23 16:58:41.296 | INFO     | __main__:train_with_distillation:358 - Batch 125/654 - loss: 0.0057
Training:  20%|█▉        | 129/654 [20:37<52:41,  6.02s/it]  2024-12-23 16:59:28.978 | INFO     | __main__:train_with_distillation:358 - Batch 130/654 - loss: 0.0054
Training:  20%|██        | 134/654 [21:24<51:46,  5.97s/it]  2024-12-23 17:00:16.022 | INFO     | __main__:train_with_distillation:358 - Batch 135/654 - loss: 0.0063
Training:  21%|██▏       | 139/654 [22:12<51:42,  6.02s/it]  2024-12-23 17:01:03.815 | INFO     | __main__:train_with_distillation:358 - Batch 140/654 - loss: 0.0046
Training:  22%|██▏       | 144/654 [23:00<51:26,  6.05s/it]  2024-12-23 17:01:51.778 | INFO     | __main__:train_with_distillation:358 - Batch 145/654 - loss: 0.0046
Training:  23%|██▎       | 149/654 [23:47<50:39,  6.02s/it]  2024-12-23 17:02:39.223 | INFO     | __main__:train_with_distillation:358 - Batch 150/654 - loss: 0.0042
Training:  24%|██▎       | 154/654 [24:34<49:58,  6.00s/it]  2024-12-23 17:03:26.535 | INFO     | __main__:train_with_distillation:358 - Batch 155/654 - loss: 0.0051
Training:  24%|██▍       | 159/654 [25:22<49:35,  6.01s/it]  2024-12-23 17:04:14.114 | INFO     | __main__:train_with_distillation:358 - Batch 160/654 - loss: 0.0070
Training:  25%|██▌       | 164/654 [26:10<49:04,  6.01s/it]  2024-12-23 17:05:01.594 | INFO     | __main__:train_with_distillation:358 - Batch 165/654 - loss: 0.0048
Training:  26%|██▌       | 169/654 [26:57<48:19,  5.98s/it]  2024-12-23 17:05:48.728 | INFO     | __main__:train_with_distillation:358 - Batch 170/654 - loss: 0.0050
Training:  27%|██▋       | 174/654 [27:44<48:10,  6.02s/it]  2024-12-23 17:06:36.497 | INFO     | __main__:train_with_distillation:358 - Batch 175/654 - loss: 0.0044
Training:  27%|██▋       | 179/654 [28:32<47:47,  6.04s/it]  2024-12-23 17:07:24.267 | INFO     | __main__:train_with_distillation:358 - Batch 180/654 - loss: 0.0044
Training:  28%|██▊       | 184/654 [29:20<47:21,  6.05s/it]  2024-12-23 17:08:12.097 | INFO     | __main__:train_with_distillation:358 - Batch 185/654 - loss: 0.0048
Training:  29%|██▉       | 189/654 [30:07<46:21,  5.98s/it]  2024-12-23 17:08:59.125 | INFO     | __main__:train_with_distillation:358 - Batch 190/654 - loss: 0.0040
Training:  30%|██▉       | 194/654 [30:54<45:20,  5.91s/it]  2024-12-23 17:09:45.604 | INFO     | __main__:train_with_distillation:358 - Batch 195/654 - loss: 0.0054
Training:  30%|███       | 199/654 [31:40<44:37,  5.88s/it]  2024-12-23 17:10:31.992 | INFO     | __main__:train_with_distillation:358 - Batch 200/654 - loss: 0.0036
Training:  31%|███       | 204/654 [32:27<44:44,  5.97s/it]  2024-12-23 17:11:19.479 | INFO     | __main__:train_with_distillation:358 - Batch 205/654 - loss: 0.0034
Training:  32%|███▏      | 209/654 [33:15<44:23,  5.98s/it]  2024-12-23 17:12:06.861 | INFO     | __main__:train_with_distillation:358 - Batch 210/654 - loss: 0.0033
Training:  33%|███▎      | 214/654 [34:02<43:48,  5.97s/it]  2024-12-23 17:12:54.021 | INFO     | __main__:train_with_distillation:358 - Batch 215/654 - loss: 0.0039
Training:  33%|███▎      | 219/654 [34:51<44:38,  6.16s/it]  2024-12-23 17:13:43.473 | INFO     | __main__:train_with_distillation:358 - Batch 220/654 - loss: 0.0027
Training:  34%|███▍      | 224/654 [35:53<51:34,  7.20s/it]  2024-12-23 17:14:44.683 | INFO     | __main__:train_with_distillation:358 - Batch 225/654 - loss: 0.0025
Training:  35%|███▌      | 229/654 [36:56<54:45,  7.73s/it]  2024-12-23 17:15:48.074 | INFO     | __main__:train_with_distillation:358 - Batch 230/654 - loss: 0.0027
Training:  36%|███▌      | 234/654 [37:59<55:08,  7.88s/it]  2024-12-23 17:16:51.009 | INFO     | __main__:train_with_distillation:358 - Batch 235/654 - loss: 0.0029
Training:  37%|███▋      | 239/654 [39:04<56:00,  8.10s/it]  2024-12-23 17:17:55.986 | INFO     | __main__:train_with_distillation:358 - Batch 240/654 - loss: 0.0033
Training:  37%|███▋      | 244/654 [40:08<55:10,  8.07s/it]  2024-12-23 17:18:59.785 | INFO     | __main__:train_with_distillation:358 - Batch 245/654 - loss: 0.0032
Training:  38%|███▊      | 249/654 [41:10<53:50,  7.98s/it]  2024-12-23 17:20:02.504 | INFO     | __main__:train_with_distillation:358 - Batch 250/654 - loss: 0.0024
Training:  39%|███▉      | 254/654 [42:15<53:53,  8.08s/it]  2024-12-23 17:21:06.911 | INFO     | __main__:train_with_distillation:358 - Batch 255/654 - loss: 0.0024
Training:  40%|███▉      | 259/654 [43:18<52:58,  8.05s/it]  2024-12-23 17:22:10.455 | INFO     | __main__:train_with_distillation:358 - Batch 260/654 - loss: 0.0024
Training:  40%|████      | 264/654 [44:22<52:01,  8.00s/it]  2024-12-23 17:23:13.608 | INFO     | __main__:train_with_distillation:358 - Batch 265/654 - loss: 0.0024
Training:  41%|████      | 269/654 [45:20<48:52,  7.62s/it]  2024-12-23 17:24:12.268 | INFO     | __main__:train_with_distillation:358 - Batch 270/654 - loss: 0.0033
Training:  42%|████▏     | 274/654 [46:22<49:00,  7.74s/it]  2024-12-23 17:25:14.008 | INFO     | __main__:train_with_distillation:358 - Batch 275/654 - loss: 0.0027
Training:  43%|████▎     | 279/654 [47:21<47:04,  7.53s/it]  2024-12-23 17:26:12.756 | INFO     | __main__:train_with_distillation:358 - Batch 280/654 - loss: 0.0021
Training:  43%|████▎     | 284/654 [48:25<48:41,  7.90s/it]  2024-12-23 17:27:16.733 | INFO     | __main__:train_with_distillation:358 - Batch 285/654 - loss: 0.0017
Training:  44%|████▍     | 289/654 [49:28<48:34,  7.99s/it]  2024-12-23 17:28:20.306 | INFO     | __main__:train_with_distillation:358 - Batch 290/654 - loss: 0.0018
Training:  45%|████▍     | 294/654 [50:28<46:08,  7.69s/it]  2024-12-23 17:29:19.883 | INFO     | __main__:train_with_distillation:358 - Batch 295/654 - loss: 0.0027
Training:  46%|████▌     | 299/654 [51:31<46:32,  7.87s/it]  2024-12-23 17:30:22.901 | INFO     | __main__:train_with_distillation:358 - Batch 300/654 - loss: 0.0047
Training:  46%|████▋     | 304/654 [52:29<43:48,  7.51s/it]  2024-12-23 17:31:20.854 | INFO     | __main__:train_with_distillation:358 - Batch 305/654 - loss: 0.0034
Training:  47%|████▋     | 309/654 [53:31<44:20,  7.71s/it]  2024-12-23 17:32:22.684 | INFO     | __main__:train_with_distillation:358 - Batch 310/654 - loss: 0.0042
Training:  48%|████▊     | 314/654 [54:17<36:50,  6.50s/it]2024-12-23 17:33:09.102 | INFO     | __main__:train_with_distillation:358 - Batch 315/654 - loss: 0.0037
Training:  49%|████▉     | 319/654 [55:11<37:19,  6.69s/it]2024-12-23 17:34:02.741 | INFO     | __main__:train_with_distillation:358 - Batch 320/654 - loss: 0.0031
Training:  50%|████▉     | 324/654 [56:05<37:38,  6.85s/it]2024-12-23 17:34:57.539 | INFO     | __main__:train_with_distillation:358 - Batch 325/654 - loss: 0.0031
Training:  50%|█████     | 329/654 [56:52<33:39,  6.21s/it]2024-12-23 17:35:44.060 | INFO     | __main__:train_with_distillation:358 - Batch 330/654 - loss: 0.0025
Training:  51%|█████     | 334/654 [57:40<32:39,  6.12s/it]2024-12-23 17:36:32.102 | INFO     | __main__:train_with_distillation:358 - Batch 335/654 - loss: 0.0024
Training:  52%|█████▏    | 339/654 [58:36<35:17,  6.72s/it]2024-12-23 17:37:27.708 | INFO     | __main__:train_with_distillation:358 - Batch 340/654 - loss: 0.0022
Training:  53%|█████▎    | 344/654 [59:34<37:07,  7.19s/it]2024-12-23 17:38:26.497 | INFO     | __main__:train_with_distillation:358 - Batch 345/654 - loss: 0.0022
Training:  53%|█████▎    | 349/654 [1:00:36<38:42,  7.61s/it]2024-12-23 17:39:28.490 | INFO     | __main__:train_with_distillation:358 - Batch 350/654 - loss: 0.0020
Training:  54%|█████▍    | 354/654 [1:01:36<37:52,  7.57s/it]2024-12-23 17:40:28.263 | INFO     | __main__:train_with_distillation:358 - Batch 355/654 - loss: 0.0021
Training:  55%|█████▍    | 359/654 [1:02:39<38:33,  7.84s/it]2024-12-23 17:41:31.437 | INFO     | __main__:train_with_distillation:358 - Batch 360/654 - loss: 0.0021
Training:  56%|█████▌    | 364/654 [1:03:40<37:24,  7.74s/it]2024-12-23 17:42:32.225 | INFO     | __main__:train_with_distillation:358 - Batch 365/654 - loss: 0.0017
Training:  56%|█████▋    | 369/654 [1:04:40<36:12,  7.62s/it]2024-12-23 17:43:32.023 | INFO     | __main__:train_with_distillation:358 - Batch 370/654 - loss: 0.0017
Training:  57%|█████▋    | 374/654 [1:05:43<36:26,  7.81s/it]2024-12-23 17:44:34.585 | INFO     | __main__:train_with_distillation:358 - Batch 375/654 - loss: 0.0023
Training:  58%|█████▊    | 379/654 [1:06:40<34:20,  7.49s/it]2024-12-23 17:45:32.553 | INFO     | __main__:train_with_distillation:358 - Batch 380/654 - loss: 0.0022
Training:  59%|█████▊    | 384/654 [1:07:44<35:18,  7.85s/it]2024-12-23 17:46:36.111 | INFO     | __main__:train_with_distillation:358 - Batch 385/654 - loss: 0.0019
Training:  59%|█████▉    | 389/654 [1:08:43<33:28,  7.58s/it]2024-12-23 17:47:34.951 | INFO     | __main__:train_with_distillation:358 - Batch 390/654 - loss: 0.0018
Training:  60%|██████    | 394/654 [1:09:44<33:17,  7.68s/it]2024-12-23 17:48:36.128 | INFO     | __main__:train_with_distillation:358 - Batch 395/654 - loss: 0.0218
Training:  61%|██████    | 399/654 [1:10:44<32:25,  7.63s/it]2024-12-23 17:49:36.302 | INFO     | __main__:train_with_distillation:358 - Batch 400/654 - loss: 0.0017
Training:  62%|██████▏   | 404/654 [1:11:45<31:51,  7.64s/it]2024-12-23 17:50:36.850 | INFO     | __main__:train_with_distillation:358 - Batch 405/654 - loss: 0.0025
Training:  63%|██████▎   | 409/654 [1:12:48<31:59,  7.83s/it]2024-12-23 17:51:39.603 | INFO     | __main__:train_with_distillation:358 - Batch 410/654 - loss: 0.0019
Training:  63%|██████▎   | 414/654 [1:13:50<31:36,  7.90s/it]2024-12-23 17:52:42.423 | INFO     | __main__:train_with_distillation:358 - Batch 415/654 - loss: 0.0018
Training:  64%|██████▍   | 419/654 [1:14:44<28:05,  7.17s/it]2024-12-23 17:53:36.145 | INFO     | __main__:train_with_distillation:358 - Batch 420/654 - loss: 0.0020
Training:  65%|██████▍   | 424/654 [1:15:43<28:16,  7.38s/it]2024-12-23 17:54:35.364 | INFO     | __main__:train_with_distillation:358 - Batch 425/654 - loss: 0.0014
Training:  66%|██████▌   | 429/654 [1:16:44<28:25,  7.58s/it]2024-12-23 17:55:36.154 | INFO     | __main__:train_with_distillation:358 - Batch 430/654 - loss: 0.0018
Training:  66%|██████▋   | 434/654 [1:17:44<27:47,  7.58s/it]2024-12-23 17:56:36.159 | INFO     | __main__:train_with_distillation:358 - Batch 435/654 - loss: 0.0015
Training:  67%|██████▋   | 439/654 [1:18:45<27:34,  7.70s/it]2024-12-23 17:57:37.520 | INFO     | __main__:train_with_distillation:358 - Batch 440/654 - loss: 0.0014
Training:  68%|██████▊   | 444/654 [1:19:47<27:04,  7.74s/it]2024-12-23 17:58:38.917 | INFO     | __main__:train_with_distillation:358 - Batch 445/654 - loss: 0.0015
Training:  69%|██████▊   | 449/654 [1:20:47<26:07,  7.65s/it]2024-12-23 17:59:39.055 | INFO     | __main__:train_with_distillation:358 - Batch 450/654 - loss: 0.0015
Training:  69%|██████▉   | 454/654 [1:21:46<25:11,  7.56s/it]2024-12-23 18:00:38.447 | INFO     | __main__:train_with_distillation:358 - Batch 455/654 - loss: 0.0018
Training:  70%|███████   | 459/654 [1:22:39<22:33,  6.94s/it]2024-12-23 18:01:30.757 | INFO     | __main__:train_with_distillation:358 - Batch 460/654 - loss: 0.0013
Training:  71%|███████   | 464/654 [1:23:32<21:25,  6.77s/it]2024-12-23 18:02:23.571 | INFO     | __main__:train_with_distillation:358 - Batch 465/654 - loss: 0.0017
Training:  72%|███████▏  | 469/654 [1:24:34<23:05,  7.49s/it]2024-12-23 18:03:25.804 | INFO     | __main__:train_with_distillation:358 - Batch 470/654 - loss: 0.0015
Training:  72%|███████▏  | 474/654 [1:25:39<23:59,  8.00s/it]2024-12-23 18:04:31.179 | INFO     | __main__:train_with_distillation:358 - Batch 475/654 - loss: 0.0014
Training:  73%|███████▎  | 479/654 [1:26:32<20:54,  7.17s/it]2024-12-23 18:05:24.481 | INFO     | __main__:train_with_distillation:358 - Batch 480/654 - loss: 0.0015
Training:  74%|███████▍  | 484/654 [1:27:19<17:55,  6.33s/it]2024-12-23 18:06:11.024 | INFO     | __main__:train_with_distillation:358 - Batch 485/654 - loss: 0.0018
Training:  75%|███████▍  | 489/654 [1:28:15<18:47,  6.83s/it]2024-12-23 18:07:07.136 | INFO     | __main__:train_with_distillation:358 - Batch 490/654 - loss: 0.0017
Training:  76%|███████▌  | 494/654 [1:29:19<20:29,  7.68s/it]2024-12-23 18:08:11.474 | INFO     | __main__:train_with_distillation:358 - Batch 495/654 - loss: 0.0019
Training:  76%|███████▋  | 499/654 [1:30:17<19:09,  7.41s/it]2024-12-23 18:09:09.021 | INFO     | __main__:train_with_distillation:358 - Batch 500/654 - loss: 0.0012
Training:  77%|███████▋  | 504/654 [1:31:03<16:01,  6.41s/it]2024-12-23 18:09:55.535 | INFO     | __main__:train_with_distillation:358 - Batch 505/654 - loss: 0.0019
Training:  78%|███████▊  | 509/654 [1:31:57<16:02,  6.64s/it]2024-12-23 18:10:49.001 | INFO     | __main__:train_with_distillation:358 - Batch 510/654 - loss: 0.0018
Training:  79%|███████▊  | 514/654 [1:32:59<17:19,  7.42s/it]2024-12-23 18:11:50.994 | INFO     | __main__:train_with_distillation:358 - Batch 515/654 - loss: 0.0013
Training:  79%|███████▉  | 519/654 [1:33:58<16:47,  7.46s/it]2024-12-23 18:12:50.177 | INFO     | __main__:train_with_distillation:358 - Batch 520/654 - loss: 0.0060
Training:  80%|████████  | 524/654 [1:34:55<15:49,  7.30s/it]2024-12-23 18:13:47.323 | INFO     | __main__:train_with_distillation:358 - Batch 525/654 - loss: 0.0013
Training:  81%|████████  | 529/654 [1:35:47<14:07,  6.78s/it]2024-12-23 18:14:38.739 | INFO     | __main__:train_with_distillation:358 - Batch 530/654 - loss: 0.0015
Training:  82%|████████▏ | 534/654 [1:36:49<14:58,  7.49s/it]2024-12-23 18:15:40.922 | INFO     | __main__:train_with_distillation:358 - Batch 535/654 - loss: 0.0015
Training:  82%|████████▏ | 539/654 [1:37:39<12:58,  6.77s/it]2024-12-23 18:16:31.443 | INFO     | __main__:train_with_distillation:358 - Batch 540/654 - loss: 0.0015
Training:  83%|████████▎ | 544/654 [1:38:27<11:28,  6.26s/it]2024-12-23 18:17:18.851 | INFO     | __main__:train_with_distillation:358 - Batch 545/654 - loss: 0.0015
Training:  84%|████████▍ | 549/654 [1:39:20<11:32,  6.60s/it]2024-12-23 18:18:12.423 | INFO     | __main__:train_with_distillation:358 - Batch 550/654 - loss: 0.0014
Training:  85%|████████▍ | 554/654 [1:40:08<10:21,  6.22s/it]2024-12-23 18:18:59.988 | INFO     | __main__:train_with_distillation:358 - Batch 555/654 - loss: 0.0016
Training:  85%|████████▌ | 559/654 [1:41:05<10:50,  6.85s/it]2024-12-23 18:19:56.808 | INFO     | __main__:train_with_distillation:358 - Batch 560/654 - loss: 0.0013
Training:  86%|████████▌ | 564/654 [1:42:05<11:02,  7.36s/it]2024-12-23 18:20:57.192 | INFO     | __main__:train_with_distillation:358 - Batch 565/654 - loss: 0.0015
Training:  87%|████████▋ | 569/654 [1:43:06<10:43,  7.57s/it]2024-12-23 18:21:57.992 | INFO     | __main__:train_with_distillation:358 - Batch 570/654 - loss: 0.0013
Training:  88%|████████▊ | 574/654 [1:44:10<10:32,  7.91s/it]2024-12-23 18:23:01.962 | INFO     | __main__:train_with_distillation:358 - Batch 575/654 - loss: 0.0013
Training:  89%|████████▊ | 579/654 [1:45:12<09:52,  7.90s/it]2024-12-23 18:24:04.351 | INFO     | __main__:train_with_distillation:358 - Batch 580/654 - loss: 0.0011
Training:  89%|████████▉ | 584/654 [1:46:11<08:53,  7.62s/it]2024-12-23 18:25:03.547 | INFO     | __main__:train_with_distillation:358 - Batch 585/654 - loss: 0.0011
Training:  90%|█████████ | 589/654 [1:47:14<08:26,  7.80s/it]2024-12-23 18:26:05.927 | INFO     | __main__:train_with_distillation:358 - Batch 590/654 - loss: 0.0011
Training:  91%|█████████ | 594/654 [1:48:13<07:35,  7.59s/it]2024-12-23 18:27:05.085 | INFO     | __main__:train_with_distillation:358 - Batch 595/654 - loss: 0.0011
Training:  92%|█████████▏| 599/654 [1:49:14<07:02,  7.68s/it]2024-12-23 18:28:06.278 | INFO     | __main__:train_with_distillation:358 - Batch 600/654 - loss: 0.0010
Training:  92%|█████████▏| 604/654 [1:50:14<06:19,  7.58s/it]2024-12-23 18:29:05.836 | INFO     | __main__:train_with_distillation:358 - Batch 605/654 - loss: 0.0012
Training:  93%|█████████▎| 609/654 [1:51:13<05:39,  7.55s/it]2024-12-23 18:30:05.425 | INFO     | __main__:train_with_distillation:358 - Batch 610/654 - loss: 0.0011
Training:  94%|█████████▍| 614/654 [1:52:12<04:58,  7.45s/it]2024-12-23 18:31:04.016 | INFO     | __main__:train_with_distillation:358 - Batch 615/654 - loss: 0.0020
Training:  95%|█████████▍| 619/654 [1:53:02<03:55,  6.74s/it]2024-12-23 18:31:54.324 | INFO     | __main__:train_with_distillation:358 - Batch 620/654 - loss: 0.0020
Training:  95%|█████████▌| 624/654 [1:53:50<03:09,  6.30s/it]2024-12-23 18:32:42.327 | INFO     | __main__:train_with_distillation:358 - Batch 625/654 - loss: 0.0018
Training:  96%|█████████▌| 629/654 [1:54:47<02:51,  6.87s/it]2024-12-23 18:33:38.967 | INFO     | __main__:train_with_distillation:358 - Batch 630/654 - loss: 0.0018
Training:  97%|█████████▋| 634/654 [1:55:49<02:29,  7.48s/it]2024-12-23 18:34:40.732 | INFO     | __main__:train_with_distillation:358 - Batch 635/654 - loss: 0.0015
Training:  98%|█████████▊| 639/654 [1:56:48<01:52,  7.52s/it]2024-12-23 18:35:40.356 | INFO     | __main__:train_with_distillation:358 - Batch 640/654 - loss: 0.0015
Training:  98%|█████████▊| 644/654 [1:57:49<01:16,  7.64s/it]2024-12-23 18:36:41.286 | INFO     | __main__:train_with_distillation:358 - Batch 645/654 - loss: 0.0017
Training:  99%|█████████▉| 649/654 [1:58:52<00:39,  7.80s/it]2024-12-23 18:37:43.673 | INFO     | __main__:train_with_distillation:358 - Batch 650/654 - loss: 0.0013
Training: 100%|█████████▉| 652/654 [1:59:55<00:24, 12.03s/it]2024-12-23 18:38:47.603 | INFO     | __main__:train_with_distillation:358 - Batch 654/654 - loss: 0.0024
Training: 100%|██████████| 654/654 [2:01:00<00:00, 11.10s/it]

Process finished with exit code 0
