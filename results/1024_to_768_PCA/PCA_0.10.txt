F:\Anaconda\envs\pytorch\python.exe F:/NLP大作业/SimCSE-Pytorch-master/ESimCSE/tongyi_distill_train.py
2024-12-23 13:38:56.023 | INFO     | __main__:<module>:501 - Starting training process with knowledge distillation from Tongyi embeddings.
2024-12-23 13:38:56.023 | INFO     | __main__:<module>:502 - Namespace(batch_size=16, data_path='../data/STS-B/', device='cuda:0', dropout=0.15, dup_rate=0.15, lr=3e-05, max_length=50, pooler='first-last-avg', pretrain_model_path='F:\\models\\bert-base-chinese', q_size=64, save_path='./model_save', teacher_save_path='./tongyi_embeddings.json')
Using cuda:0 device.

2024-12-23 13:38:56.304 | INFO     | __main__:<module>:507 - Test Embeddings长度: 1024
2024-12-23 13:38:56.327 | INFO     | __main__:main:430 - Generating/updating embeddings...
2024-12-23 13:38:59.206 | INFO     | __main__:generate_teacher_embeddings:156 - Total sentences: 10462, Remaining to embed: 0
Generating embeddings: 0it [00:00, ?it/s]
2024-12-23 13:38:59.208 | INFO     | __main__:generate_teacher_embeddings:183 - Embedding generation completed. Saved to ./tongyi_embeddings.json
2024-12-23 13:39:02.888 | INFO     | __main__:train_with_distillation:312 - Applying PCA to teacher embeddings...
Training:   0%|          | 0/654 [00:00<?, ?it/s]F:\Anaconda\envs\pytorch\lib\site-packages\transformers\models\bert\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
Training:   0%|          | 3/654 [00:55<2:36:14, 14.40s/it] 2024-12-23 13:39:59.214 | INFO     | __main__:train_with_distillation:348 - Batch 5/654 - loss: 0.1096
Training:   0%|          | 3/654 [01:05<2:36:14, 14.40s/it]2024-12-23 13:41:10.220 | INFO     | __main__:train_with_distillation:354 - New best corrcoef: 0.5981 at batch 5, model saved.
Training:   1%|▏         | 9/654 [02:06<1:38:10,  9.13s/it]2024-12-23 13:41:10.587 | INFO     | __main__:train_with_distillation:348 - Batch 10/654 - loss: 0.0355
Training:   2%|▏         | 14/654 [03:14<1:33:09,  8.73s/it]2024-12-23 13:42:18.229 | INFO     | __main__:train_with_distillation:348 - Batch 15/654 - loss: 0.0279
Training:   3%|▎         | 19/654 [04:23<1:32:40,  8.76s/it]2024-12-23 13:43:27.617 | INFO     | __main__:train_with_distillation:348 - Batch 20/654 - loss: 0.0260
Training:   3%|▎         | 19/654 [04:35<1:32:40,  8.76s/it]2024-12-23 13:44:33.627 | INFO     | __main__:train_with_distillation:354 - New best corrcoef: 0.6008 at batch 20, model saved.
Training:   4%|▎         | 24/654 [05:30<1:29:22,  8.51s/it]2024-12-23 13:44:33.993 | INFO     | __main__:train_with_distillation:348 - Batch 25/654 - loss: 0.0238
Training:   4%|▎         | 24/654 [05:45<1:29:22,  8.51s/it]2024-12-23 13:45:42.535 | INFO     | __main__:train_with_distillation:354 - New best corrcoef: 0.6130 at batch 25, model saved.
Training:   4%|▍         | 29/654 [06:39<1:29:59,  8.64s/it]2024-12-23 13:45:42.907 | INFO     | __main__:train_with_distillation:348 - Batch 30/654 - loss: 0.0237
Training:   4%|▍         | 29/654 [06:55<1:29:59,  8.64s/it]2024-12-23 13:46:52.092 | INFO     | __main__:train_with_distillation:354 - New best corrcoef: 0.6185 at batch 30, model saved.
Training:   5%|▌         | 34/654 [07:48<1:30:16,  8.74s/it]2024-12-23 13:46:52.460 | INFO     | __main__:train_with_distillation:348 - Batch 35/654 - loss: 0.0216
Training:   6%|▌         | 39/654 [08:55<1:27:37,  8.55s/it]2024-12-23 13:47:59.347 | INFO     | __main__:train_with_distillation:348 - Batch 40/654 - loss: 0.0194
Training:   7%|▋         | 44/654 [09:56<1:21:13,  7.99s/it]2024-12-23 13:49:00.254 | INFO     | __main__:train_with_distillation:348 - Batch 45/654 - loss: 0.0173
Training:   7%|▋         | 49/654 [10:56<1:17:30,  7.69s/it]2024-12-23 13:49:59.826 | INFO     | __main__:train_with_distillation:348 - Batch 50/654 - loss: 0.1415
Training:   8%|▊         | 54/654 [11:44<1:06:41,  6.67s/it]2024-12-23 13:50:48.328 | INFO     | __main__:train_with_distillation:348 - Batch 55/654 - loss: 0.0161
Training:   9%|▉         | 59/654 [12:33<1:03:13,  6.38s/it]2024-12-23 13:51:37.541 | INFO     | __main__:train_with_distillation:348 - Batch 60/654 - loss: 0.0147
Training:  10%|▉         | 64/654 [13:33<1:10:31,  7.17s/it]2024-12-23 13:52:37.575 | INFO     | __main__:train_with_distillation:348 - Batch 65/654 - loss: 0.0128
Training:  11%|█         | 69/654 [14:33<1:11:56,  7.38s/it]2024-12-23 13:53:36.773 | INFO     | __main__:train_with_distillation:348 - Batch 70/654 - loss: 0.0146
Training:  11%|█         | 69/654 [14:46<1:11:56,  7.38s/it]2024-12-23 13:54:35.430 | INFO     | __main__:train_with_distillation:354 - New best corrcoef: 0.6320 at batch 70, model saved.
Training:  11%|█▏        | 74/654 [15:32<1:11:50,  7.43s/it]2024-12-23 13:54:35.797 | INFO     | __main__:train_with_distillation:348 - Batch 75/654 - loss: 0.0109
Training:  12%|█▏        | 79/654 [16:35<1:14:31,  7.78s/it]2024-12-23 13:55:38.729 | INFO     | __main__:train_with_distillation:348 - Batch 80/654 - loss: 0.0099
Training:  13%|█▎        | 84/654 [17:31<1:09:49,  7.35s/it]2024-12-23 13:56:35.103 | INFO     | __main__:train_with_distillation:348 - Batch 85/654 - loss: 0.0093
Training:  14%|█▎        | 89/654 [18:26<1:06:55,  7.11s/it]2024-12-23 13:57:30.291 | INFO     | __main__:train_with_distillation:348 - Batch 90/654 - loss: 0.0086
Training:  14%|█▍        | 94/654 [19:24<1:07:41,  7.25s/it]2024-12-23 13:58:28.287 | INFO     | __main__:train_with_distillation:348 - Batch 95/654 - loss: 0.0069
Training:  15%|█▌        | 99/654 [20:21<1:07:04,  7.25s/it]2024-12-23 13:59:25.649 | INFO     | __main__:train_with_distillation:348 - Batch 100/654 - loss: 0.0080
Training:  16%|█▌        | 104/654 [21:20<1:07:31,  7.37s/it]2024-12-23 14:00:24.402 | INFO     | __main__:train_with_distillation:348 - Batch 105/654 - loss: 0.0074
Training:  17%|█▋        | 109/654 [22:20<1:08:13,  7.51s/it]2024-12-23 14:01:24.425 | INFO     | __main__:train_with_distillation:348 - Batch 110/654 - loss: 0.0075
Training:  17%|█▋        | 114/654 [23:19<1:06:43,  7.41s/it]2024-12-23 14:02:22.672 | INFO     | __main__:train_with_distillation:348 - Batch 115/654 - loss: 0.0076
Training:  17%|█▋        | 114/654 [23:36<1:06:43,  7.41s/it]2024-12-23 14:03:26.932 | INFO     | __main__:train_with_distillation:354 - New best corrcoef: 0.6463 at batch 115, model saved.
Training:  18%|█▊        | 119/654 [24:23<1:10:31,  7.91s/it]2024-12-23 14:03:27.302 | INFO     | __main__:train_with_distillation:348 - Batch 120/654 - loss: 0.0096
Training:  19%|█▉        | 124/654 [25:24<1:08:29,  7.75s/it]2024-12-23 14:04:27.994 | INFO     | __main__:train_with_distillation:348 - Batch 125/654 - loss: 0.0096
Training:  20%|█▉        | 129/654 [26:20<1:04:24,  7.36s/it]2024-12-23 14:05:24.617 | INFO     | __main__:train_with_distillation:348 - Batch 130/654 - loss: 0.0075
Training:  20%|██        | 134/654 [27:18<1:03:10,  7.29s/it]2024-12-23 14:06:21.978 | INFO     | __main__:train_with_distillation:348 - Batch 135/654 - loss: 0.0073
Training:  21%|██▏       | 139/654 [28:20<1:05:38,  7.65s/it]2024-12-23 14:07:23.952 | INFO     | __main__:train_with_distillation:348 - Batch 140/654 - loss: 0.0065
Training:  22%|██▏       | 144/654 [29:17<1:02:22,  7.34s/it]2024-12-23 14:08:20.726 | INFO     | __main__:train_with_distillation:348 - Batch 145/654 - loss: 0.0061
Training:  23%|██▎       | 149/654 [30:15<1:02:02,  7.37s/it]2024-12-23 14:09:19.163 | INFO     | __main__:train_with_distillation:348 - Batch 150/654 - loss: 0.0055
Training:  24%|██▎       | 154/654 [31:17<1:03:51,  7.66s/it]2024-12-23 14:10:21.018 | INFO     | __main__:train_with_distillation:348 - Batch 155/654 - loss: 0.0053
Training:  24%|██▍       | 159/654 [32:17<1:02:44,  7.61s/it]2024-12-23 14:11:20.957 | INFO     | __main__:train_with_distillation:348 - Batch 160/654 - loss: 0.0042
Training:  25%|██▌       | 164/654 [33:04<53:03,  6.50s/it]  2024-12-23 14:12:07.714 | INFO     | __main__:train_with_distillation:348 - Batch 165/654 - loss: 0.0047
Training:  26%|██▌       | 169/654 [34:01<56:41,  7.01s/it]  2024-12-23 14:13:05.318 | INFO     | __main__:train_with_distillation:348 - Batch 170/654 - loss: 0.0036
Training:  27%|██▋       | 174/654 [35:01<59:00,  7.38s/it]  2024-12-23 14:14:05.173 | INFO     | __main__:train_with_distillation:348 - Batch 175/654 - loss: 0.0035
Training:  27%|██▋       | 179/654 [35:59<58:23,  7.38s/it]  2024-12-23 14:15:03.521 | INFO     | __main__:train_with_distillation:348 - Batch 180/654 - loss: 0.0040
Training:  28%|██▊       | 184/654 [36:58<57:55,  7.39s/it]  2024-12-23 14:16:02.095 | INFO     | __main__:train_with_distillation:348 - Batch 185/654 - loss: 0.0032
Training:  29%|██▉       | 189/654 [37:56<56:46,  7.33s/it]  2024-12-23 14:16:59.745 | INFO     | __main__:train_with_distillation:348 - Batch 190/654 - loss: 0.0029
Training:  30%|██▉       | 194/654 [38:55<57:01,  7.44s/it]  2024-12-23 14:17:59.070 | INFO     | __main__:train_with_distillation:348 - Batch 195/654 - loss: 0.0586
Training:  30%|███       | 199/654 [39:55<57:28,  7.58s/it]  2024-12-23 14:18:59.628 | INFO     | __main__:train_with_distillation:348 - Batch 200/654 - loss: 0.0042
Training:  31%|███       | 204/654 [40:53<55:22,  7.38s/it]  2024-12-23 14:19:57.230 | INFO     | __main__:train_with_distillation:348 - Batch 205/654 - loss: 0.0045
Training:  32%|███▏      | 209/654 [41:51<54:22,  7.33s/it]  2024-12-23 14:20:54.986 | INFO     | __main__:train_with_distillation:348 - Batch 210/654 - loss: 0.0037
Training:  33%|███▎      | 214/654 [42:47<52:47,  7.20s/it]  2024-12-23 14:21:51.390 | INFO     | __main__:train_with_distillation:348 - Batch 215/654 - loss: 0.0037
Training:  33%|███▎      | 219/654 [43:54<57:57,  7.99s/it]  2024-12-23 14:22:57.952 | INFO     | __main__:train_with_distillation:348 - Batch 220/654 - loss: 0.0031
Training:  34%|███▍      | 224/654 [44:52<54:04,  7.54s/it]  2024-12-23 14:23:55.748 | INFO     | __main__:train_with_distillation:348 - Batch 225/654 - loss: 0.0031
Training:  35%|███▌      | 229/654 [45:50<52:27,  7.41s/it]  2024-12-23 14:24:53.748 | INFO     | __main__:train_with_distillation:348 - Batch 230/654 - loss: 0.0029
Training:  36%|███▌      | 234/654 [46:36<44:55,  6.42s/it]  2024-12-23 14:25:40.399 | INFO     | __main__:train_with_distillation:348 - Batch 235/654 - loss: 0.0031
Training:  37%|███▋      | 239/654 [47:30<46:17,  6.69s/it]  2024-12-23 14:26:34.469 | INFO     | __main__:train_with_distillation:348 - Batch 240/654 - loss: 0.0027
Training:  37%|███▋      | 244/654 [48:17<42:12,  6.18s/it]  2024-12-23 14:27:21.176 | INFO     | __main__:train_with_distillation:348 - Batch 245/654 - loss: 0.0032
Training:  38%|███▊      | 249/654 [49:04<40:40,  6.02s/it]  2024-12-23 14:28:08.149 | INFO     | __main__:train_with_distillation:348 - Batch 250/654 - loss: 0.0035
Training:  39%|███▉      | 254/654 [49:57<43:01,  6.45s/it]  2024-12-23 14:29:00.974 | INFO     | __main__:train_with_distillation:348 - Batch 255/654 - loss: 0.0032
Training:  40%|███▉      | 259/654 [50:56<46:59,  7.14s/it]  2024-12-23 14:30:00.242 | INFO     | __main__:train_with_distillation:348 - Batch 260/654 - loss: 0.0028
Training:  40%|████      | 264/654 [51:47<43:11,  6.64s/it]  2024-12-23 14:30:50.728 | INFO     | __main__:train_with_distillation:348 - Batch 265/654 - loss: 0.0029
Training:  41%|████      | 269/654 [52:38<42:08,  6.57s/it]  2024-12-23 14:31:42.351 | INFO     | __main__:train_with_distillation:348 - Batch 270/654 - loss: 0.0030
Training:  42%|████▏     | 274/654 [53:39<46:02,  7.27s/it]  2024-12-23 14:32:42.755 | INFO     | __main__:train_with_distillation:348 - Batch 275/654 - loss: 0.0027
Training:  43%|████▎     | 279/654 [54:36<45:28,  7.28s/it]  2024-12-23 14:33:40.347 | INFO     | __main__:train_with_distillation:348 - Batch 280/654 - loss: 0.0024
Training:  43%|████▎     | 284/654 [55:33<44:15,  7.18s/it]  2024-12-23 14:34:36.718 | INFO     | __main__:train_with_distillation:348 - Batch 285/654 - loss: 0.0021
Training:  44%|████▍     | 289/654 [56:34<45:47,  7.53s/it]  2024-12-23 14:35:37.712 | INFO     | __main__:train_with_distillation:348 - Batch 290/654 - loss: 0.0017
Training:  45%|████▍     | 294/654 [57:32<44:28,  7.41s/it]  2024-12-23 14:36:35.873 | INFO     | __main__:train_with_distillation:348 - Batch 295/654 - loss: 0.0030
Training:  46%|████▌     | 299/654 [58:30<43:49,  7.41s/it]  2024-12-23 14:37:34.462 | INFO     | __main__:train_with_distillation:348 - Batch 300/654 - loss: 0.0026
Training:  46%|████▋     | 304/654 [59:28<42:55,  7.36s/it]  2024-12-23 14:38:32.492 | INFO     | __main__:train_with_distillation:348 - Batch 305/654 - loss: 0.0028
Training:  47%|████▋     | 309/654 [1:00:28<43:13,  7.52s/it]  2024-12-23 14:39:32.614 | INFO     | __main__:train_with_distillation:348 - Batch 310/654 - loss: 0.0031
Training:  48%|████▊     | 314/654 [1:01:25<41:19,  7.29s/it]  2024-12-23 14:40:29.364 | INFO     | __main__:train_with_distillation:348 - Batch 315/654 - loss: 0.0023
Training:  49%|████▉     | 319/654 [1:02:21<39:50,  7.14s/it]  2024-12-23 14:41:25.167 | INFO     | __main__:train_with_distillation:348 - Batch 320/654 - loss: 0.0025
Training:  50%|████▉     | 324/654 [1:03:12<36:44,  6.68s/it]2024-12-23 14:42:16.116 | INFO     | __main__:train_with_distillation:348 - Batch 325/654 - loss: 0.0020
Training:  50%|█████     | 329/654 [1:04:00<34:00,  6.28s/it]2024-12-23 14:43:04.089 | INFO     | __main__:train_with_distillation:348 - Batch 330/654 - loss: 0.0079
Training:  51%|█████     | 334/654 [1:05:02<39:07,  7.34s/it]2024-12-23 14:44:06.516 | INFO     | __main__:train_with_distillation:348 - Batch 335/654 - loss: 0.0026
Training:  52%|█████▏    | 339/654 [1:05:59<37:49,  7.20s/it]2024-12-23 14:45:02.935 | INFO     | __main__:train_with_distillation:348 - Batch 340/654 - loss: 0.0040
Training:  53%|█████▎    | 344/654 [1:06:56<37:18,  7.22s/it]2024-12-23 14:46:00.131 | INFO     | __main__:train_with_distillation:348 - Batch 345/654 - loss: 0.0027
Training:  53%|█████▎    | 349/654 [1:07:55<37:34,  7.39s/it]2024-12-23 14:46:59.324 | INFO     | __main__:train_with_distillation:348 - Batch 350/654 - loss: 0.0024
Training:  54%|█████▍    | 354/654 [1:08:52<36:28,  7.30s/it]2024-12-23 14:47:56.647 | INFO     | __main__:train_with_distillation:348 - Batch 355/654 - loss: 0.0028
Training:  55%|█████▍    | 359/654 [1:09:49<35:18,  7.18s/it]2024-12-23 14:48:52.977 | INFO     | __main__:train_with_distillation:348 - Batch 360/654 - loss: 0.0018
Training:  56%|█████▌    | 364/654 [1:10:48<35:36,  7.37s/it]2024-12-23 14:49:52.010 | INFO     | __main__:train_with_distillation:348 - Batch 365/654 - loss: 0.0023
Training:  56%|█████▋    | 369/654 [1:11:46<34:49,  7.33s/it]2024-12-23 14:50:49.835 | INFO     | __main__:train_with_distillation:348 - Batch 370/654 - loss: 0.0018
Training:  57%|█████▋    | 374/654 [1:12:43<34:06,  7.31s/it]2024-12-23 14:51:47.568 | INFO     | __main__:train_with_distillation:348 - Batch 375/654 - loss: 0.0016
Training:  58%|█████▊    | 379/654 [1:13:45<35:04,  7.65s/it]2024-12-23 14:52:49.567 | INFO     | __main__:train_with_distillation:348 - Batch 380/654 - loss: 0.0021
Training:  59%|█████▊    | 384/654 [1:14:42<33:02,  7.34s/it]2024-12-23 14:53:46.322 | INFO     | __main__:train_with_distillation:348 - Batch 385/654 - loss: 0.0017
Training:  59%|█████▉    | 389/654 [1:15:41<32:49,  7.43s/it]2024-12-23 14:54:45.509 | INFO     | __main__:train_with_distillation:348 - Batch 390/654 - loss: 0.0014
Training:  60%|██████    | 394/654 [1:16:37<30:54,  7.13s/it]2024-12-23 14:55:40.680 | INFO     | __main__:train_with_distillation:348 - Batch 395/654 - loss: 0.0013
Training:  61%|██████    | 399/654 [1:17:35<31:08,  7.33s/it]2024-12-23 14:56:39.458 | INFO     | __main__:train_with_distillation:348 - Batch 400/654 - loss: 0.0020
Training:  62%|██████▏   | 404/654 [1:18:35<31:09,  7.48s/it]2024-12-23 14:57:39.248 | INFO     | __main__:train_with_distillation:348 - Batch 405/654 - loss: 0.0018
Training:  63%|██████▎   | 409/654 [1:19:34<30:21,  7.43s/it]2024-12-23 14:58:37.862 | INFO     | __main__:train_with_distillation:348 - Batch 410/654 - loss: 0.0017
Training:  63%|██████▎   | 414/654 [1:20:31<29:15,  7.32s/it]2024-12-23 14:59:35.260 | INFO     | __main__:train_with_distillation:348 - Batch 415/654 - loss: 0.0017
Training:  64%|██████▍   | 419/654 [1:21:29<28:41,  7.33s/it]2024-12-23 15:00:33.232 | INFO     | __main__:train_with_distillation:348 - Batch 420/654 - loss: 0.0014
Training:  65%|██████▍   | 424/654 [1:22:27<27:57,  7.29s/it]2024-12-23 15:01:30.806 | INFO     | __main__:train_with_distillation:348 - Batch 425/654 - loss: 0.0015
Training:  66%|██████▌   | 429/654 [1:23:23<26:53,  7.17s/it]2024-12-23 15:02:27.014 | INFO     | __main__:train_with_distillation:348 - Batch 430/654 - loss: 0.0012
Training:  66%|██████▋   | 434/654 [1:24:22<26:58,  7.36s/it]2024-12-23 15:03:25.990 | INFO     | __main__:train_with_distillation:348 - Batch 435/654 - loss: 0.0012
Training:  67%|██████▋   | 439/654 [1:25:19<26:03,  7.27s/it]2024-12-23 15:04:23.173 | INFO     | __main__:train_with_distillation:348 - Batch 440/654 - loss: 0.0013
Training:  68%|██████▊   | 444/654 [1:26:16<25:10,  7.19s/it]2024-12-23 15:05:19.754 | INFO     | __main__:train_with_distillation:348 - Batch 445/654 - loss: 0.0017
Training:  69%|██████▊   | 449/654 [1:27:15<25:13,  7.38s/it]2024-12-23 15:06:18.934 | INFO     | __main__:train_with_distillation:348 - Batch 450/654 - loss: 0.0023
Training:  69%|██████▉   | 454/654 [1:28:12<24:19,  7.30s/it]2024-12-23 15:07:16.309 | INFO     | __main__:train_with_distillation:348 - Batch 455/654 - loss: 0.0021
Training:  70%|███████   | 459/654 [1:29:11<23:54,  7.35s/it]2024-12-23 15:08:14.714 | INFO     | __main__:train_with_distillation:348 - Batch 460/654 - loss: 0.0041
Training:  71%|███████   | 464/654 [1:30:11<23:52,  7.54s/it]2024-12-23 15:09:15.114 | INFO     | __main__:train_with_distillation:348 - Batch 465/654 - loss: 0.0025
Training:  72%|███████▏  | 469/654 [1:31:11<23:19,  7.56s/it]2024-12-23 15:10:15.036 | INFO     | __main__:train_with_distillation:348 - Batch 470/654 - loss: 0.0023
Training:  72%|███████▏  | 474/654 [1:32:08<22:04,  7.36s/it]2024-12-23 15:11:12.428 | INFO     | __main__:train_with_distillation:348 - Batch 475/654 - loss: 0.0032
Training:  73%|███████▎  | 479/654 [1:33:06<21:17,  7.30s/it]2024-12-23 15:12:09.969 | INFO     | __main__:train_with_distillation:348 - Batch 480/654 - loss: 0.0020
Training:  74%|███████▍  | 484/654 [1:34:04<20:44,  7.32s/it]2024-12-23 15:13:07.966 | INFO     | __main__:train_with_distillation:348 - Batch 485/654 - loss: 0.0019
Training:  75%|███████▍  | 489/654 [1:35:02<20:06,  7.31s/it]2024-12-23 15:14:05.758 | INFO     | __main__:train_with_distillation:348 - Batch 490/654 - loss: 0.0017
Training:  76%|███████▌  | 494/654 [1:36:04<20:35,  7.72s/it]2024-12-23 15:15:08.544 | INFO     | __main__:train_with_distillation:348 - Batch 495/654 - loss: 0.0019
Training:  76%|███████▋  | 499/654 [1:37:00<18:41,  7.23s/it]2024-12-23 15:16:03.725 | INFO     | __main__:train_with_distillation:348 - Batch 500/654 - loss: 0.0020
Training:  77%|███████▋  | 504/654 [1:37:50<16:44,  6.70s/it]2024-12-23 15:16:54.464 | INFO     | __main__:train_with_distillation:348 - Batch 505/654 - loss: 0.0015
Training:  78%|███████▊  | 509/654 [1:38:44<16:23,  6.78s/it]2024-12-23 15:17:48.469 | INFO     | __main__:train_with_distillation:348 - Batch 510/654 - loss: 0.0015
Training:  79%|███████▊  | 514/654 [1:39:43<16:47,  7.20s/it]2024-12-23 15:18:47.099 | INFO     | __main__:train_with_distillation:348 - Batch 515/654 - loss: 0.0014
Training:  79%|███████▉  | 519/654 [1:40:35<15:12,  6.76s/it]2024-12-23 15:19:38.766 | INFO     | __main__:train_with_distillation:348 - Batch 520/654 - loss: 0.0015
Training:  80%|████████  | 524/654 [1:41:23<13:41,  6.32s/it]2024-12-23 15:20:26.908 | INFO     | __main__:train_with_distillation:348 - Batch 525/654 - loss: 0.0045
Training:  81%|████████  | 529/654 [1:42:19<14:19,  6.87s/it]2024-12-23 15:21:23.546 | INFO     | __main__:train_with_distillation:348 - Batch 530/654 - loss: 0.0054
Training:  82%|████████▏ | 534/654 [1:43:19<14:34,  7.29s/it]2024-12-23 15:22:22.933 | INFO     | __main__:train_with_distillation:348 - Batch 535/654 - loss: 0.0048
Training:  82%|████████▏ | 539/654 [1:44:17<14:04,  7.35s/it]2024-12-23 15:23:21.281 | INFO     | __main__:train_with_distillation:348 - Batch 540/654 - loss: 0.0026
Training:  83%|████████▎ | 544/654 [1:45:09<12:28,  6.80s/it]2024-12-23 15:24:12.853 | INFO     | __main__:train_with_distillation:348 - Batch 545/654 - loss: 0.0249
Training:  84%|████████▍ | 549/654 [1:46:05<12:19,  7.04s/it]2024-12-23 15:25:09.511 | INFO     | __main__:train_with_distillation:348 - Batch 550/654 - loss: 0.0024
Training:  85%|████████▍ | 554/654 [1:47:06<12:25,  7.46s/it]2024-12-23 15:26:10.242 | INFO     | __main__:train_with_distillation:348 - Batch 555/654 - loss: 0.0035
Training:  85%|████████▌ | 559/654 [1:48:03<11:35,  7.32s/it]2024-12-23 15:27:07.632 | INFO     | __main__:train_with_distillation:348 - Batch 560/654 - loss: 0.0032
Training:  86%|████████▌ | 564/654 [1:48:59<10:44,  7.16s/it]2024-12-23 15:28:03.597 | INFO     | __main__:train_with_distillation:348 - Batch 565/654 - loss: 0.0034
Training:  87%|████████▋ | 569/654 [1:49:56<10:09,  7.17s/it]2024-12-23 15:29:00.387 | INFO     | __main__:train_with_distillation:348 - Batch 570/654 - loss: 0.0028
Training:  88%|████████▊ | 574/654 [1:50:53<09:32,  7.16s/it]2024-12-23 15:29:56.970 | INFO     | __main__:train_with_distillation:348 - Batch 575/654 - loss: 0.0021
Training:  89%|████████▊ | 579/654 [1:51:52<09:15,  7.40s/it]2024-12-23 15:30:56.566 | INFO     | __main__:train_with_distillation:348 - Batch 580/654 - loss: 0.0020
Training:  89%|████████▉ | 584/654 [1:52:57<09:15,  7.94s/it]2024-12-23 15:32:01.555 | INFO     | __main__:train_with_distillation:348 - Batch 585/654 - loss: 0.0019
Training:  90%|█████████ | 589/654 [1:53:54<08:03,  7.44s/it]2024-12-23 15:32:58.318 | INFO     | __main__:train_with_distillation:348 - Batch 590/654 - loss: 0.0024
Training:  91%|█████████ | 594/654 [1:54:55<07:34,  7.58s/it]2024-12-23 15:33:58.881 | INFO     | __main__:train_with_distillation:348 - Batch 595/654 - loss: 0.0025
Training:  92%|█████████▏| 599/654 [1:55:50<06:36,  7.21s/it]2024-12-23 15:34:54.350 | INFO     | __main__:train_with_distillation:348 - Batch 600/654 - loss: 0.0331
Training:  92%|█████████▏| 604/654 [1:56:40<05:30,  6.61s/it]2024-12-23 15:35:44.187 | INFO     | __main__:train_with_distillation:348 - Batch 605/654 - loss: 0.0017
Training:  93%|█████████▎| 609/654 [1:57:27<04:36,  6.15s/it]2024-12-23 15:36:30.949 | INFO     | __main__:train_with_distillation:348 - Batch 610/654 - loss: 0.0023
Training:  94%|█████████▍| 614/654 [1:58:14<04:00,  6.00s/it]2024-12-23 15:37:17.724 | INFO     | __main__:train_with_distillation:348 - Batch 615/654 - loss: 0.0020
Training:  95%|█████████▍| 619/654 [1:59:01<03:29,  5.97s/it]2024-12-23 15:38:04.823 | INFO     | __main__:train_with_distillation:348 - Batch 620/654 - loss: 0.0018
Training:  95%|█████████▌| 624/654 [1:59:48<02:59,  5.97s/it]2024-12-23 15:38:51.935 | INFO     | __main__:train_with_distillation:348 - Batch 625/654 - loss: 0.0016
Training:  96%|█████████▌| 629/654 [2:00:35<02:28,  5.95s/it]2024-12-23 15:39:38.892 | INFO     | __main__:train_with_distillation:348 - Batch 630/654 - loss: 0.0017
Training:  97%|█████████▋| 634/654 [2:01:22<01:58,  5.95s/it]2024-12-23 15:40:25.892 | INFO     | __main__:train_with_distillation:348 - Batch 635/654 - loss: 0.0015
Training:  98%|█████████▊| 639/654 [2:02:09<01:29,  5.98s/it]2024-12-23 15:41:13.280 | INFO     | __main__:train_with_distillation:348 - Batch 640/654 - loss: 0.0013
Training:  98%|█████████▊| 644/654 [2:02:56<00:59,  5.95s/it]2024-12-23 15:42:00.251 | INFO     | __main__:train_with_distillation:348 - Batch 645/654 - loss: 0.0019
Training:  99%|█████████▉| 649/654 [2:03:43<00:29,  5.96s/it]2024-12-23 15:42:47.419 | INFO     | __main__:train_with_distillation:348 - Batch 650/654 - loss: 0.0014
Training: 100%|█████████▉| 652/654 [2:04:30<00:17,  8.95s/it]2024-12-23 15:43:34.353 | INFO     | __main__:train_with_distillation:348 - Batch 654/654 - loss: 0.0013
Training: 100%|██████████| 654/654 [2:05:18<00:00, 11.50s/it]

Process finished with exit code 0
