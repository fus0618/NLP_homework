F:\Anaconda\envs\pytorch\python.exe F:/NLP大作业/SimCSE-Pytorch-master/ESimCSE/tongyi_distill_train.py
2024-12-23 11:14:52.827 | INFO     | __main__:<module>:445 - Starting training process with knowledge distillation from Tongyi embeddings.
2024-12-23 11:14:52.827 | INFO     | __main__:<module>:446 - Namespace(batch_size=16, data_path='../data/STS-B/', device='cuda:0', dropout=0.15, dup_rate=0.15, lr=3e-05, max_length=50, pooler='first-last-avg', pretrain_model_path='F:\\models\\bert-base-chinese', q_size=64, save_path='./model_save', teacher_save_path='./tongyi_embeddings.json')
Using cuda:0 device.

2024-12-23 11:14:53.074 | INFO     | __main__:<module>:451 - Test Embeddings长度: 1024
2024-12-23 11:14:53.096 | INFO     | __main__:main:374 - Generating/updating embeddings...
2024-12-23 11:14:56.026 | INFO     | __main__:generate_teacher_embeddings:156 - Total sentences: 10462, Remaining to embed: 0
Generating embeddings: 0it [00:00, ?it/s]
2024-12-23 11:14:56.028 | INFO     | __main__:generate_teacher_embeddings:183 - Embedding generation completed. Saved to ./tongyi_embeddings.json
Training:   0%|          | 0/654 [00:00<?, ?it/s]F:\Anaconda\envs\pytorch\lib\site-packages\transformers\models\bert\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
Training:   0%|          | 3/654 [00:57<2:43:02, 15.03s/it] 2024-12-23 11:15:57.820 | INFO     | __main__:train_with_distillation:292 - Batch 5/654 - loss: 0.0896
Training:   0%|          | 3/654 [01:16<2:43:02, 15.03s/it]2024-12-23 11:17:24.446 | INFO     | __main__:train_with_distillation:298 - New best corrcoef: 0.5522 at batch 5, model saved.
Training:   1%|▏         | 9/654 [02:24<1:54:50, 10.68s/it]2024-12-23 11:17:24.815 | INFO     | __main__:train_with_distillation:292 - Batch 10/654 - loss: 0.0549
Training:   1%|▏         | 9/654 [02:36<1:54:50, 10.68s/it]2024-12-23 11:18:37.957 | INFO     | __main__:train_with_distillation:298 - New best corrcoef: 0.5873 at batch 10, model saved.
Training:   2%|▏         | 14/654 [03:38<1:43:49,  9.73s/it]2024-12-23 11:18:38.332 | INFO     | __main__:train_with_distillation:292 - Batch 15/654 - loss: 0.0367
Training:   2%|▏         | 14/654 [03:56<1:43:49,  9.73s/it]2024-12-23 11:19:30.939 | INFO     | __main__:train_with_distillation:298 - New best corrcoef: 0.6021 at batch 15, model saved.
Training:   3%|▎         | 19/654 [04:31<1:21:39,  7.72s/it]2024-12-23 11:19:31.307 | INFO     | __main__:train_with_distillation:292 - Batch 20/654 - loss: 0.0342
Training:   3%|▎         | 19/654 [04:46<1:21:39,  7.72s/it]2024-12-23 11:20:26.418 | INFO     | __main__:train_with_distillation:298 - New best corrcoef: 0.6084 at batch 20, model saved.
Training:   4%|▎         | 24/654 [05:26<1:16:10,  7.25s/it]2024-12-23 11:20:26.786 | INFO     | __main__:train_with_distillation:292 - Batch 25/654 - loss: 0.0261
Training:   4%|▎         | 24/654 [05:46<1:16:10,  7.25s/it]2024-12-23 11:21:38.832 | INFO     | __main__:train_with_distillation:298 - New best corrcoef: 0.6092 at batch 25, model saved.
Training:   4%|▍         | 29/654 [06:39<1:28:32,  8.50s/it]2024-12-23 11:21:39.203 | INFO     | __main__:train_with_distillation:292 - Batch 30/654 - loss: 0.0313
Training:   4%|▍         | 29/654 [06:56<1:28:32,  8.50s/it]2024-12-23 11:22:26.309 | INFO     | __main__:train_with_distillation:298 - New best corrcoef: 0.6248 at batch 30, model saved.
Training:   5%|▌         | 34/654 [07:26<1:10:52,  6.86s/it]2024-12-23 11:22:26.677 | INFO     | __main__:train_with_distillation:292 - Batch 35/654 - loss: 0.0258
Training:   5%|▌         | 34/654 [07:46<1:10:52,  6.86s/it]2024-12-23 11:23:20.900 | INFO     | __main__:train_with_distillation:298 - New best corrcoef: 0.6281 at batch 35, model saved.
Training:   6%|▌         | 39/654 [08:21<1:10:36,  6.89s/it]2024-12-23 11:23:21.267 | INFO     | __main__:train_with_distillation:292 - Batch 40/654 - loss: 0.0240
Training:   7%|▋         | 44/654 [09:25<1:17:53,  7.66s/it]2024-12-23 11:24:25.104 | INFO     | __main__:train_with_distillation:292 - Batch 45/654 - loss: 0.0196
Training:   7%|▋         | 49/654 [10:33<1:23:29,  8.28s/it]2024-12-23 11:25:33.208 | INFO     | __main__:train_with_distillation:292 - Batch 50/654 - loss: 0.0185
Training:   8%|▊         | 54/654 [11:30<1:16:02,  7.60s/it]2024-12-23 11:26:30.547 | INFO     | __main__:train_with_distillation:292 - Batch 55/654 - loss: 0.0182
Training:   9%|▉         | 59/654 [12:37<1:20:32,  8.12s/it]2024-12-23 11:27:36.989 | INFO     | __main__:train_with_distillation:292 - Batch 60/654 - loss: 0.0173
Training:  10%|▉         | 64/654 [13:36<1:16:03,  7.73s/it]2024-12-23 11:28:36.574 | INFO     | __main__:train_with_distillation:292 - Batch 65/654 - loss: 0.0137
Training:  11%|█         | 69/654 [14:26<1:05:57,  6.77s/it]2024-12-23 11:29:26.063 | INFO     | __main__:train_with_distillation:292 - Batch 70/654 - loss: 0.0125
Training:  11%|█▏        | 74/654 [15:24<1:09:01,  7.14s/it]2024-12-23 11:30:24.106 | INFO     | __main__:train_with_distillation:292 - Batch 75/654 - loss: 0.0120
Training:  12%|█▏        | 79/654 [16:21<1:09:18,  7.23s/it]2024-12-23 11:31:21.688 | INFO     | __main__:train_with_distillation:292 - Batch 80/654 - loss: 0.0103
Training:  13%|█▎        | 84/654 [17:19<1:09:18,  7.30s/it]2024-12-23 11:32:19.676 | INFO     | __main__:train_with_distillation:292 - Batch 85/654 - loss: 0.0107
Training:  14%|█▎        | 89/654 [18:24<1:13:52,  7.85s/it]2024-12-23 11:33:24.029 | INFO     | __main__:train_with_distillation:292 - Batch 90/654 - loss: 0.0088
Training:  14%|█▍        | 94/654 [19:24<1:11:46,  7.69s/it]2024-12-23 11:34:24.241 | INFO     | __main__:train_with_distillation:292 - Batch 95/654 - loss: 0.0083
Training:  15%|█▌        | 99/654 [20:24<1:10:19,  7.60s/it]2024-12-23 11:35:24.021 | INFO     | __main__:train_with_distillation:292 - Batch 100/654 - loss: 0.0074
Training:  16%|█▌        | 104/654 [21:21<1:07:53,  7.41s/it]2024-12-23 11:36:21.791 | INFO     | __main__:train_with_distillation:292 - Batch 105/654 - loss: 0.0066
Training:  17%|█▋        | 109/654 [22:21<1:08:20,  7.52s/it]2024-12-23 11:37:21.804 | INFO     | __main__:train_with_distillation:292 - Batch 110/654 - loss: 0.0061
Training:  17%|█▋        | 114/654 [23:26<1:11:20,  7.93s/it]2024-12-23 11:38:26.190 | INFO     | __main__:train_with_distillation:292 - Batch 115/654 - loss: 0.0058
Training:  18%|█▊        | 119/654 [24:30<1:11:38,  8.04s/it]2024-12-23 11:39:30.155 | INFO     | __main__:train_with_distillation:292 - Batch 120/654 - loss: 0.0052
Training:  19%|█▉        | 124/654 [25:32<1:10:18,  7.96s/it]2024-12-23 11:40:32.809 | INFO     | __main__:train_with_distillation:292 - Batch 125/654 - loss: 0.0042
Training:  20%|█▉        | 129/654 [26:31<1:06:23,  7.59s/it]2024-12-23 11:41:31.262 | INFO     | __main__:train_with_distillation:292 - Batch 130/654 - loss: 0.0044
Training:  20%|██        | 134/654 [27:31<1:05:46,  7.59s/it]2024-12-23 11:42:31.325 | INFO     | __main__:train_with_distillation:292 - Batch 135/654 - loss: 0.0043
Training:  21%|██▏       | 139/654 [28:33<1:06:47,  7.78s/it]2024-12-23 11:43:33.694 | INFO     | __main__:train_with_distillation:292 - Batch 140/654 - loss: 0.0038
Training:  22%|██▏       | 144/654 [29:40<1:09:31,  8.18s/it]2024-12-23 11:44:40.055 | INFO     | __main__:train_with_distillation:292 - Batch 145/654 - loss: 0.0424
Training:  23%|██▎       | 149/654 [30:43<1:07:52,  8.06s/it]2024-12-23 11:45:43.404 | INFO     | __main__:train_with_distillation:292 - Batch 150/654 - loss: 0.0049
Training:  24%|██▎       | 154/654 [31:43<1:04:26,  7.73s/it]2024-12-23 11:46:43.204 | INFO     | __main__:train_with_distillation:292 - Batch 155/654 - loss: 0.0063
Training:  24%|██▍       | 159/654 [32:48<1:06:44,  8.09s/it]2024-12-23 11:47:48.681 | INFO     | __main__:train_with_distillation:292 - Batch 160/654 - loss: 0.0052
Training:  25%|██▌       | 164/654 [33:53<1:06:34,  8.15s/it]2024-12-23 11:48:53.444 | INFO     | __main__:train_with_distillation:292 - Batch 165/654 - loss: 0.0068
Training:  26%|██▌       | 169/654 [34:56<1:05:00,  8.04s/it]2024-12-23 11:49:56.619 | INFO     | __main__:train_with_distillation:292 - Batch 170/654 - loss: 0.0046
Training:  27%|██▋       | 174/654 [36:00<1:04:19,  8.04s/it]2024-12-23 11:51:00.243 | INFO     | __main__:train_with_distillation:292 - Batch 175/654 - loss: 0.0041
Training:  27%|██▋       | 179/654 [37:04<1:03:58,  8.08s/it]2024-12-23 11:52:04.360 | INFO     | __main__:train_with_distillation:292 - Batch 180/654 - loss: 0.0036
Training:  28%|██▊       | 184/654 [38:01<58:54,  7.52s/it]  2024-12-23 11:53:01.517 | INFO     | __main__:train_with_distillation:292 - Batch 185/654 - loss: 0.0032
Training:  29%|██▉       | 189/654 [38:55<54:29,  7.03s/it]  2024-12-23 11:53:55.100 | INFO     | __main__:train_with_distillation:292 - Batch 190/654 - loss: 0.0035
Training:  30%|██▉       | 194/654 [39:59<59:14,  7.73s/it]  2024-12-23 11:54:59.120 | INFO     | __main__:train_with_distillation:292 - Batch 195/654 - loss: 0.0028
Training:  30%|███       | 199/654 [41:01<58:59,  7.78s/it]  2024-12-23 11:56:00.888 | INFO     | __main__:train_with_distillation:292 - Batch 200/654 - loss: 0.0025
Training:  31%|███       | 204/654 [42:01<57:28,  7.66s/it]  2024-12-23 11:57:01.036 | INFO     | __main__:train_with_distillation:292 - Batch 205/654 - loss: 0.0028
Training:  32%|███▏      | 209/654 [42:52<51:16,  6.91s/it]  2024-12-23 11:57:52.592 | INFO     | __main__:train_with_distillation:292 - Batch 210/654 - loss: 0.0025
Training:  33%|███▎      | 214/654 [43:41<47:12,  6.44s/it]  2024-12-23 11:58:41.521 | INFO     | __main__:train_with_distillation:292 - Batch 215/654 - loss: 0.0021
Training:  33%|███▎      | 219/654 [44:37<49:30,  6.83s/it]  2024-12-23 11:59:37.160 | INFO     | __main__:train_with_distillation:292 - Batch 220/654 - loss: 0.0030
Training:  34%|███▍      | 224/654 [45:38<53:20,  7.44s/it]  2024-12-23 12:00:38.577 | INFO     | __main__:train_with_distillation:292 - Batch 225/654 - loss: 0.0025
Training:  35%|███▌      | 229/654 [46:35<51:20,  7.25s/it]  2024-12-23 12:01:35.114 | INFO     | __main__:train_with_distillation:292 - Batch 230/654 - loss: 0.0021
Training:  36%|███▌      | 234/654 [47:34<51:57,  7.42s/it]  2024-12-23 12:02:34.539 | INFO     | __main__:train_with_distillation:292 - Batch 235/654 - loss: 0.0021
Training:  37%|███▋      | 239/654 [48:37<53:41,  7.76s/it]  2024-12-23 12:03:37.356 | INFO     | __main__:train_with_distillation:292 - Batch 240/654 - loss: 0.0024
Training:  37%|███▋      | 244/654 [49:40<54:02,  7.91s/it]  2024-12-23 12:04:40.526 | INFO     | __main__:train_with_distillation:292 - Batch 245/654 - loss: 0.0032
Training:  38%|███▊      | 249/654 [50:42<52:48,  7.82s/it]  2024-12-23 12:05:42.069 | INFO     | __main__:train_with_distillation:292 - Batch 250/654 - loss: 0.0028
Training:  39%|███▉      | 254/654 [51:42<51:20,  7.70s/it]  2024-12-23 12:06:42.485 | INFO     | __main__:train_with_distillation:292 - Batch 255/654 - loss: 0.0025
Training:  40%|███▉      | 259/654 [52:44<51:02,  7.75s/it]  2024-12-23 12:07:44.042 | INFO     | __main__:train_with_distillation:292 - Batch 260/654 - loss: 0.0021
Training:  40%|████      | 264/654 [53:45<50:31,  7.77s/it]  2024-12-23 12:08:45.626 | INFO     | __main__:train_with_distillation:292 - Batch 265/654 - loss: 0.0017
Training:  41%|████      | 269/654 [54:45<49:10,  7.66s/it]  2024-12-23 12:09:45.798 | INFO     | __main__:train_with_distillation:292 - Batch 270/654 - loss: 0.0026
Training:  42%|████▏     | 274/654 [55:48<49:26,  7.81s/it]  2024-12-23 12:10:48.149 | INFO     | __main__:train_with_distillation:292 - Batch 275/654 - loss: 0.0024
Training:  43%|████▎     | 279/654 [56:47<47:40,  7.63s/it]  2024-12-23 12:11:47.753 | INFO     | __main__:train_with_distillation:292 - Batch 280/654 - loss: 0.0021
Training:  43%|████▎     | 284/654 [57:48<47:22,  7.68s/it]  2024-12-23 12:12:48.758 | INFO     | __main__:train_with_distillation:292 - Batch 285/654 - loss: 0.0018
Training:  44%|████▍     | 289/654 [58:53<48:27,  7.97s/it]  2024-12-23 12:13:52.968 | INFO     | __main__:train_with_distillation:292 - Batch 290/654 - loss: 0.0018
Training:  45%|████▍     | 294/654 [59:56<48:04,  8.01s/it]  2024-12-23 12:14:56.546 | INFO     | __main__:train_with_distillation:292 - Batch 295/654 - loss: 0.0018
Training:  46%|████▌     | 299/654 [1:00:56<45:51,  7.75s/it]  2024-12-23 12:15:56.774 | INFO     | __main__:train_with_distillation:292 - Batch 300/654 - loss: 0.0018
Training:  46%|████▋     | 304/654 [1:01:46<39:46,  6.82s/it]  2024-12-23 12:16:46.822 | INFO     | __main__:train_with_distillation:292 - Batch 305/654 - loss: 0.0018
Training:  47%|████▋     | 309/654 [1:02:46<42:01,  7.31s/it]  2024-12-23 12:17:46.672 | INFO     | __main__:train_with_distillation:292 - Batch 310/654 - loss: 0.0017
Training:  48%|████▊     | 314/654 [1:03:43<40:44,  7.19s/it]  2024-12-23 12:18:43.049 | INFO     | __main__:train_with_distillation:292 - Batch 315/654 - loss: 0.0015
Training:  49%|████▉     | 319/654 [1:04:36<38:20,  6.87s/it]2024-12-23 12:19:36.015 | INFO     | __main__:train_with_distillation:292 - Batch 320/654 - loss: 0.0018
Training:  50%|████▉     | 324/654 [1:05:37<41:04,  7.47s/it]  2024-12-23 12:20:37.580 | INFO     | __main__:train_with_distillation:292 - Batch 325/654 - loss: 0.0017
Training:  50%|█████     | 329/654 [1:06:35<39:52,  7.36s/it]  2024-12-23 12:21:35.379 | INFO     | __main__:train_with_distillation:292 - Batch 330/654 - loss: 0.0015
Training:  51%|█████     | 334/654 [1:07:39<41:50,  7.85s/it]  2024-12-23 12:22:39.442 | INFO     | __main__:train_with_distillation:292 - Batch 335/654 - loss: 0.0022
Training:  52%|█████▏    | 339/654 [1:08:35<38:31,  7.34s/it]2024-12-23 12:23:35.396 | INFO     | __main__:train_with_distillation:292 - Batch 340/654 - loss: 0.0018
Training:  53%|█████▎    | 344/654 [1:09:32<37:17,  7.22s/it]2024-12-23 12:24:31.988 | INFO     | __main__:train_with_distillation:292 - Batch 345/654 - loss: 0.0015
Training:  53%|█████▎    | 349/654 [1:10:29<36:42,  7.22s/it]2024-12-23 12:25:29.156 | INFO     | __main__:train_with_distillation:292 - Batch 350/654 - loss: 0.0015
Training:  54%|█████▍    | 354/654 [1:11:31<38:16,  7.66s/it]2024-12-23 12:26:31.520 | INFO     | __main__:train_with_distillation:292 - Batch 355/654 - loss: 0.0011
Training:  55%|█████▍    | 359/654 [1:12:26<35:27,  7.21s/it]2024-12-23 12:27:26.714 | INFO     | __main__:train_with_distillation:292 - Batch 360/654 - loss: 0.0011
Training:  56%|█████▌    | 364/654 [1:13:27<36:15,  7.50s/it]2024-12-23 12:28:27.291 | INFO     | __main__:train_with_distillation:292 - Batch 365/654 - loss: 0.0015
Training:  56%|█████▋    | 369/654 [1:14:25<35:10,  7.41s/it]2024-12-23 12:29:25.478 | INFO     | __main__:train_with_distillation:292 - Batch 370/654 - loss: 0.0053
Training:  57%|█████▋    | 374/654 [1:15:26<35:15,  7.56s/it]2024-12-23 12:30:25.877 | INFO     | __main__:train_with_distillation:292 - Batch 375/654 - loss: 0.0024
Training:  58%|█████▊    | 379/654 [1:16:22<33:24,  7.29s/it]2024-12-23 12:31:22.443 | INFO     | __main__:train_with_distillation:292 - Batch 380/654 - loss: 0.0031
Training:  59%|█████▊    | 384/654 [1:17:22<33:35,  7.46s/it]2024-12-23 12:32:22.206 | INFO     | __main__:train_with_distillation:292 - Batch 385/654 - loss: 0.0020
Training:  59%|█████▉    | 389/654 [1:18:21<32:57,  7.46s/it]2024-12-23 12:33:21.215 | INFO     | __main__:train_with_distillation:292 - Batch 390/654 - loss: 0.0021
Training:  60%|██████    | 394/654 [1:19:25<34:02,  7.86s/it]2024-12-23 12:34:25.004 | INFO     | __main__:train_with_distillation:292 - Batch 395/654 - loss: 0.0024
Training:  61%|██████    | 399/654 [1:20:29<34:13,  8.05s/it]2024-12-23 12:35:29.569 | INFO     | __main__:train_with_distillation:292 - Batch 400/654 - loss: 0.0020
Training:  62%|██████▏   | 404/654 [1:21:28<31:47,  7.63s/it]2024-12-23 12:36:28.183 | INFO     | __main__:train_with_distillation:292 - Batch 405/654 - loss: 0.0017
Training:  63%|██████▎   | 409/654 [1:22:24<29:39,  7.26s/it]2024-12-23 12:37:24.108 | INFO     | __main__:train_with_distillation:292 - Batch 410/654 - loss: 0.0018
Training:  63%|██████▎   | 414/654 [1:23:24<29:57,  7.49s/it]2024-12-23 12:38:24.296 | INFO     | __main__:train_with_distillation:292 - Batch 415/654 - loss: 0.0016
Training:  64%|██████▍   | 419/654 [1:24:27<30:28,  7.78s/it]2024-12-23 12:39:27.067 | INFO     | __main__:train_with_distillation:292 - Batch 420/654 - loss: 0.0017
Training:  65%|██████▍   | 424/654 [1:25:26<29:08,  7.60s/it]2024-12-23 12:40:26.447 | INFO     | __main__:train_with_distillation:292 - Batch 425/654 - loss: 0.0017
Training:  66%|██████▌   | 429/654 [1:26:21<27:01,  7.21s/it]2024-12-23 12:41:21.815 | INFO     | __main__:train_with_distillation:292 - Batch 430/654 - loss: 0.1012
Training:  66%|██████▋   | 434/654 [1:27:08<23:10,  6.32s/it]2024-12-23 12:42:08.115 | INFO     | __main__:train_with_distillation:292 - Batch 435/654 - loss: 0.0044
Training:  67%|██████▋   | 439/654 [1:28:05<24:40,  6.89s/it]2024-12-23 12:43:04.953 | INFO     | __main__:train_with_distillation:292 - Batch 440/654 - loss: 0.0020
Training:  68%|██████▊   | 444/654 [1:29:02<24:51,  7.10s/it]2024-12-23 12:44:02.015 | INFO     | __main__:train_with_distillation:292 - Batch 445/654 - loss: 0.0018
Training:  68%|██████▊   | 444/654 [1:29:20<24:51,  7.10s/it]2024-12-23 12:44:58.637 | INFO     | __main__:train_with_distillation:298 - New best corrcoef: 0.6304 at batch 445, model saved.
Training:  69%|██████▊   | 449/654 [1:29:59<24:29,  7.17s/it]2024-12-23 12:44:59.004 | INFO     | __main__:train_with_distillation:292 - Batch 450/654 - loss: 0.0020
Training:  69%|██████▊   | 449/654 [1:30:10<24:29,  7.17s/it]2024-12-23 12:45:58.841 | INFO     | __main__:train_with_distillation:298 - New best corrcoef: 0.6312 at batch 450, model saved.
Training:  69%|██████▉   | 454/654 [1:30:59<24:51,  7.46s/it]2024-12-23 12:45:59.211 | INFO     | __main__:train_with_distillation:292 - Batch 455/654 - loss: 0.0018
Training:  69%|██████▉   | 454/654 [1:31:10<24:51,  7.46s/it]2024-12-23 12:46:59.429 | INFO     | __main__:train_with_distillation:298 - New best corrcoef: 0.6323 at batch 455, model saved.
Training:  70%|███████   | 459/654 [1:31:59<24:39,  7.59s/it]2024-12-23 12:46:59.797 | INFO     | __main__:train_with_distillation:292 - Batch 460/654 - loss: 0.0016
Training:  70%|███████   | 459/654 [1:32:10<24:39,  7.59s/it]2024-12-23 12:48:00.607 | INFO     | __main__:train_with_distillation:298 - New best corrcoef: 0.6323 at batch 460, model saved.
Training:  71%|███████   | 464/654 [1:33:01<24:19,  7.68s/it]2024-12-23 12:48:00.977 | INFO     | __main__:train_with_distillation:292 - Batch 465/654 - loss: 0.0017
Training:  72%|███████▏  | 469/654 [1:34:00<23:18,  7.56s/it]2024-12-23 12:49:00.280 | INFO     | __main__:train_with_distillation:292 - Batch 470/654 - loss: 0.0016
Training:  72%|███████▏  | 474/654 [1:35:00<22:43,  7.57s/it]2024-12-23 12:50:00.259 | INFO     | __main__:train_with_distillation:292 - Batch 475/654 - loss: 0.0014
Training:  73%|███████▎  | 479/654 [1:36:01<22:26,  7.70s/it]2024-12-23 12:51:01.631 | INFO     | __main__:train_with_distillation:292 - Batch 480/654 - loss: 0.0016
Training:  74%|███████▍  | 484/654 [1:37:02<21:48,  7.69s/it]2024-12-23 12:52:02.506 | INFO     | __main__:train_with_distillation:292 - Batch 485/654 - loss: 0.0019
Training:  75%|███████▍  | 489/654 [1:38:03<21:03,  7.65s/it]2024-12-23 12:53:02.887 | INFO     | __main__:train_with_distillation:292 - Batch 490/654 - loss: 0.0021
Training:  76%|███████▌  | 494/654 [1:39:01<19:53,  7.46s/it]2024-12-23 12:54:01.064 | INFO     | __main__:train_with_distillation:292 - Batch 495/654 - loss: 0.0020
Training:  76%|███████▋  | 499/654 [1:39:58<18:54,  7.32s/it]2024-12-23 12:54:58.429 | INFO     | __main__:train_with_distillation:292 - Batch 500/654 - loss: 0.0034
Training:  77%|███████▋  | 504/654 [1:40:46<16:09,  6.46s/it]2024-12-23 12:55:45.962 | INFO     | __main__:train_with_distillation:292 - Batch 505/654 - loss: 0.0015
Training:  78%|███████▊  | 509/654 [1:41:33<14:56,  6.19s/it]2024-12-23 12:56:33.721 | INFO     | __main__:train_with_distillation:292 - Batch 510/654 - loss: 0.0014
Training:  79%|███████▊  | 514/654 [1:42:32<16:18,  6.99s/it]2024-12-23 12:57:32.370 | INFO     | __main__:train_with_distillation:292 - Batch 515/654 - loss: 0.0013
Training:  79%|███████▉  | 519/654 [1:43:36<17:16,  7.68s/it]2024-12-23 12:58:35.967 | INFO     | __main__:train_with_distillation:292 - Batch 520/654 - loss: 0.0017
Training:  80%|████████  | 524/654 [1:44:26<14:50,  6.85s/it]2024-12-23 12:59:26.683 | INFO     | __main__:train_with_distillation:292 - Batch 525/654 - loss: 0.0014
Training:  81%|████████  | 529/654 [1:45:16<13:26,  6.46s/it]2024-12-23 13:00:16.077 | INFO     | __main__:train_with_distillation:292 - Batch 530/654 - loss: 0.0013
Training:  82%|████████▏ | 534/654 [1:46:16<14:27,  7.23s/it]2024-12-23 13:01:16.506 | INFO     | __main__:train_with_distillation:292 - Batch 535/654 - loss: 0.0021
Training:  82%|████████▏ | 539/654 [1:47:17<14:22,  7.50s/it]2024-12-23 13:02:16.886 | INFO     | __main__:train_with_distillation:292 - Batch 540/654 - loss: 0.0023
Training:  83%|████████▎ | 544/654 [1:48:17<13:50,  7.55s/it]2024-12-23 13:03:16.873 | INFO     | __main__:train_with_distillation:292 - Batch 545/654 - loss: 0.0019
Training:  84%|████████▍ | 549/654 [1:49:08<12:01,  6.87s/it]2024-12-23 13:04:08.403 | INFO     | __main__:train_with_distillation:292 - Batch 550/654 - loss: 0.0014
Training:  85%|████████▍ | 554/654 [1:49:55<10:24,  6.24s/it]2024-12-23 13:04:55.164 | INFO     | __main__:train_with_distillation:292 - Batch 555/654 - loss: 0.0015
Training:  85%|████████▌ | 559/654 [1:50:53<11:04,  7.00s/it]2024-12-23 13:05:53.603 | INFO     | __main__:train_with_distillation:292 - Batch 560/654 - loss: 0.0023
Training:  86%|████████▌ | 564/654 [1:51:55<11:17,  7.53s/it]2024-12-23 13:06:55.382 | INFO     | __main__:train_with_distillation:292 - Batch 565/654 - loss: 0.0013
Training:  87%|████████▋ | 569/654 [1:52:50<10:06,  7.13s/it]2024-12-23 13:07:50.155 | INFO     | __main__:train_with_distillation:292 - Batch 570/654 - loss: 0.0012
Training:  88%|████████▊ | 574/654 [1:53:51<10:04,  7.56s/it]2024-12-23 13:08:51.750 | INFO     | __main__:train_with_distillation:292 - Batch 575/654 - loss: 0.0014
Training:  89%|████████▊ | 579/654 [1:54:51<09:26,  7.56s/it]2024-12-23 13:09:51.522 | INFO     | __main__:train_with_distillation:292 - Batch 580/654 - loss: 0.0013
Training:  89%|████████▉ | 584/654 [1:55:42<07:59,  6.84s/it]2024-12-23 13:10:42.691 | INFO     | __main__:train_with_distillation:292 - Batch 585/654 - loss: 0.0012
Training:  90%|█████████ | 589/654 [1:56:40<07:42,  7.12s/it]2024-12-23 13:11:40.136 | INFO     | __main__:train_with_distillation:292 - Batch 590/654 - loss: 0.0010
Training:  91%|█████████ | 594/654 [1:57:41<07:30,  7.50s/it]2024-12-23 13:12:41.084 | INFO     | __main__:train_with_distillation:292 - Batch 595/654 - loss: 0.0010
Training:  92%|█████████▏| 599/654 [1:58:43<07:05,  7.74s/it]2024-12-23 13:13:43.278 | INFO     | __main__:train_with_distillation:292 - Batch 600/654 - loss: 0.0011
Training:  92%|█████████▏| 604/654 [1:59:45<06:30,  7.80s/it]2024-12-23 13:14:45.253 | INFO     | __main__:train_with_distillation:292 - Batch 605/654 - loss: 0.0010
Training:  93%|█████████▎| 609/654 [2:00:46<05:47,  7.72s/it]2024-12-23 13:15:46.009 | INFO     | __main__:train_with_distillation:292 - Batch 610/654 - loss: 0.0009
Training:  94%|█████████▍| 614/654 [2:01:43<04:56,  7.42s/it]2024-12-23 13:16:43.399 | INFO     | __main__:train_with_distillation:292 - Batch 615/654 - loss: 0.0011
Training:  95%|█████████▍| 619/654 [2:02:41<04:16,  7.33s/it]2024-12-23 13:17:40.985 | INFO     | __main__:train_with_distillation:292 - Batch 620/654 - loss: 0.0011
Training:  95%|█████████▌| 624/654 [2:03:36<03:33,  7.11s/it]2024-12-23 13:18:36.385 | INFO     | __main__:train_with_distillation:292 - Batch 625/654 - loss: 0.0011
Training:  96%|█████████▌| 629/654 [2:04:34<03:00,  7.22s/it]2024-12-23 13:19:33.936 | INFO     | __main__:train_with_distillation:292 - Batch 630/654 - loss: 0.0011
Training:  97%|█████████▋| 634/654 [2:05:33<02:28,  7.44s/it]2024-12-23 13:20:33.722 | INFO     | __main__:train_with_distillation:292 - Batch 635/654 - loss: 0.0010
Training:  98%|█████████▊| 639/654 [2:06:32<01:51,  7.42s/it]2024-12-23 13:21:32.267 | INFO     | __main__:train_with_distillation:292 - Batch 640/654 - loss: 0.0011
Training:  98%|█████████▊| 644/654 [2:07:19<01:04,  6.43s/it]2024-12-23 13:22:18.987 | INFO     | __main__:train_with_distillation:292 - Batch 645/654 - loss: 0.0009
Training:  99%|█████████▉| 649/654 [2:08:15<00:34,  6.89s/it]2024-12-23 13:23:15.425 | INFO     | __main__:train_with_distillation:292 - Batch 650/654 - loss: 0.0015
Training: 100%|█████████▉| 652/654 [2:09:17<00:22, 11.28s/it]2024-12-23 13:24:17.157 | INFO     | __main__:train_with_distillation:292 - Batch 654/654 - loss: 0.0045
Training: 100%|██████████| 654/654 [2:10:17<00:00, 11.95s/it]

Process finished with exit code 0
